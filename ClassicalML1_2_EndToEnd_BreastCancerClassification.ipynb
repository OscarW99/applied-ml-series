{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarW99/applied-ml-series/blob/main/ClassicalML1_2_EndToEnd_BreastCancerClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VltreO7tWfGh"
      },
      "source": [
        "# Intro1.2_EndToEnd_BreastCancerClassification\n",
        "\n",
        "This notebook walks through an end-to-end machine learning project for classifying breast cancer tumors as malignant or benign using the Wisconsin Breast Cancer dataset from Scikit-learn.\n",
        "\n",
        "# Project Roadmap:\n",
        "1.  Get the data.\n",
        "2.  Take a first look (explore & visualize).\n",
        "3.  Split into training and test sets (super important!).\n",
        "4.  More exploration on the training set.\n",
        "5.  Prepare the data for our ML algorithms (cleaning, scaling).\n",
        "6.  Select and train a few models.\n",
        "7.  Fine-tune the best model.\n",
        "8.  Evaluate on the test set.\n",
        "9.  Briefly touch on saving the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaNdNN42W_gV"
      },
      "source": [
        "---\n",
        "# 1. Getting Our Hands on the Data\n",
        "\n",
        "We'll use the Wisconsin Breast Cancer dataset, which is conveniently included in Scikit-Learn. It's a classic dataset for this kind of binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF9pSAK7XAwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the dataset\n",
        "cancer = load_breast_cancer()\n",
        "print(dir(cancer))\n",
        "print(cancer.DESCR)\n",
        "print(cancer.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aol8fo-AuW1_"
      },
      "outputs": [],
      "source": [
        "# Create a Pandas DataFrame\n",
        "# Features are in cancer.data, target (diagnosis) is in cancer.target\n",
        "# Feature names are in cancer.feature_names\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMWWDxlDtT6o"
      },
      "outputs": [],
      "source": [
        "# The target in scikit-learn's dataset is often 0 for malignant and 1 for benign.\n",
        "# For clarity and common convention (1 for positive class), let's map:\n",
        "# Malignant -> 1\n",
        "# Benign -> 0\n",
        "df['diagnosis'] = (cancer.target == 0).astype(int) # 0 is malignant in original, so (cancer.target == 0) is True (1) for malignant\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Shape of the dataset: {df.shape}\")\n",
        "\n",
        "# Let's see what `cancer.target` looked like and how our `diagnosis` column is now.\n",
        "# `cancer.target`: 0 means malignant, 1 means benign.\n",
        "# Our `df['diagnosis']`: 1 means malignant, 0 means benign.\n",
        "\n",
        "print(\"\\nOriginal Scikit-learn target unique values:\", np.unique(cancer.target))\n",
        "print(\"Our 'diagnosis' column unique values:\", np.unique(df['diagnosis']))\n",
        "print(\"Counts for our 'diagnosis' column:\")\n",
        "print(df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkt2Po3UXC5z"
      },
      "source": [
        "---\n",
        "# 2. Initial Data Exploration & Visualization\n",
        "\n",
        "Let's get a first look at the data structure and feature distributions, especially how they differ by diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izTrxGbJXEdN"
      },
      "outputs": [],
      "source": [
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset info (columns, data types, non-null counts):\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nStatistical summary of numerical features:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA_BfBS0XF1i"
      },
      "source": [
        "Now, let's plot histograms for all numerical features, separated by diagnosis. This will give us a much better idea of which features might be good discriminators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc5u_DwTXHV9"
      },
      "outputs": [],
      "source": [
        "# # Plot histograms for a all features\n",
        "numerical_features = df.drop('diagnosis', axis=1).columns # Get all feature column names\n",
        "\n",
        "# Create histograms for all numerical features, separated by diagnosis\n",
        "# We'll plot a few at a time for readability, or you can loop through all.\n",
        "# Let's determine a reasonable number of rows and columns for subplots\n",
        "n_features = len(numerical_features)\n",
        "n_cols = 3 # Or 4, depending on preference\n",
        "n_rows = (n_features + n_cols - 1) // n_cols # Ceiling division\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten to easily iterate over axes\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    sns.histplot(data=df, x=feature, hue='diagnosis', kde=True, ax=axes[i], palette={0: 'skyblue', 1: 'salmon'})\n",
        "    axes[i].set_title(f'Distribution of {feature}')\n",
        "    axes[i].legend(title='Diagnosis', labels=['Malignant (1)', 'Benign (0)']) # Adjust labels based on hue order\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Feature Distributions by Diagnosis (0=Benign, 1=Malignant)\", fontsize=16, y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y75UNpgkXJxD"
      },
      "source": [
        "The histograms above show how the values for each feature are distributed for benign (skyblue) versus malignant (salmon) tumors. Features where the two colored distributions show good separation are likely to be good predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyJvWngyXLDs"
      },
      "source": [
        "---\n",
        "# 3. Split into Training and Test Sets\n",
        "\n",
        "This is a critical step! We set aside a portion of the data for final testing and do not touch it during model development or tuning. We use stratification to maintain class proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBCsk47MXMwl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Stratified split\n",
        "# test_size=0.2 means 20% for test, 80% for train\n",
        "# random_state ensures we get the same split every time we run the code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "print(\"\\nTraining set diagnosis distribution:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"\\nTest set diagnosis distribution:\")\n",
        "print(y_test.value_counts(normalize=True)) # Should be similar to training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1LmXB1mXNtX"
      },
      "source": [
        "The distributions look similar, so our stratified split worked well! Now, we lock away `X_test` and `y_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pdqyWQbXO2V"
      },
      "source": [
        "---\n",
        "# 4. Deeper Exploration & Visualization on the Training Set\n",
        "\n",
        "Now, we only work with `X_train` and `y_train`.\n",
        "\n",
        "**UMAP Visualization**\n",
        "\n",
        "Let's use UMAP to try and visualize the overall structure of our training data in 2D, colored by diagnosis. UMAP works best on scaled data.\n",
        "(You might need to install UMAP: `!pip install umap-learn`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da9SHu9gXRS2"
      },
      "outputs": [],
      "source": [
        "# For UMAP, you'd typically scale the data first\n",
        "# Make sure you have umap-learn installed: pip install umap-learn\n",
        "# You can run this in a cell: !pip install umap-learn\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from umap import UMAP\n",
        "\n",
        "    # Scale a copy of X_train for UMAP\n",
        "    X_train_scaled_for_umap = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "    reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, n_jobs=1) # n_jobs=1 for reproducibility\n",
        "    X_train_umap = reducer.fit_transform(X_train_scaled_for_umap)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # y_train == 0 is Benign, y_train == 1 is Malignant\n",
        "    plt.scatter(X_train_umap[y_train==0, 0], X_train_umap[y_train==0, 1], label=\"Benign (0)\", c=\"skyblue\", alpha=0.7)\n",
        "    plt.scatter(X_train_umap[y_train==1, 0], X_train_umap[y_train==1, 1], label=\"Malignant (1)\", c=\"salmon\", alpha=0.7)\n",
        "    plt.xlabel(\"UMAP Component 1\")\n",
        "    plt.ylabel(\"UMAP Component 2\")\n",
        "    plt.title(\"UMAP of Training Samples (Scaled) by Diagnosis\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"UMAP not installed or an issue occurred. Skipping UMAP plot.\")\n",
        "    print(\"To install UMAP: !pip install umap-learn (run this in a new code cell if needed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wblH5_ANXSi6"
      },
      "source": [
        "The UMAP plot should hopefully show some separation between the benign and malignant clusters. If they are heavily overlapped, the problem might be very hard.\n",
        "\n",
        "### **Correlation Analysis on Training Data**\n",
        "\n",
        "Let's see how features correlate with the diagnosis in our training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5lAyA4EXTZZ"
      },
      "outputs": [],
      "source": [
        "# Add diagnosis back to training set (temporarily) for correlation calculation\n",
        "X_train_with_target = X_train.copy()\n",
        "X_train_with_target['diagnosis'] = y_train\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix_train = X_train_with_target.corr()\n",
        "\n",
        "# Visualize the correlation of features with the 'diagnosis' column using a heatmap\n",
        "plt.figure(figsize=(8, 12)) # Adjusted for more features potentially\n",
        "sns.heatmap(corr_matrix_train[['diagnosis']].sort_values(by='diagnosis', ascending=False), annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation of Training Features with Diagnosis (1=Malignant)\")\n",
        "plt.show()\n",
        "\n",
        "# Print the most correlated features\n",
        "sorted_corr_train = corr_matrix_train[\"diagnosis\"].sort_values(ascending=False)\n",
        "print(\"\\nTop Positive Correlations with Diagnosis (Training Data):\")\n",
        "print(sorted_corr_train.head(10))\n",
        "print(\"\\nTop Negative Correlations (actually least positive here):\")\n",
        "print(sorted_corr_train.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh9yOfc-XU-m"
      },
      "source": [
        "Features like `worst concave points`, `worst perimeter`, `worst radius`, `mean concave points` show strong positive correlation with malignancy (diagnosis=1). This makes intuitive sense.\n",
        "\n",
        "Let's look at a scatter matrix for a few highly correlated features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSiqCHq1XVnj"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix # Already imported, but good to remember\n",
        "\n",
        "# Select top features based on correlation with diagnosis (from training data)\n",
        "# Exclude 'diagnosis' itself from the list of features to plot against each other\n",
        "top_features_for_pairplot = [f for f in sorted_corr_train.index if f != 'diagnosis'][:4]\n",
        "\n",
        "print(f\"\\nTop features selected for PairPlot: {top_features_for_pairplot}\")\n",
        "\n",
        "if top_features_for_pairplot: # Proceed only if features were selected\n",
        "    # sns.pairplot creates its own figure, so plt.figure might not be needed or could interfere.\n",
        "    sns.pairplot(X_train_with_target, vars=top_features_for_pairplot, hue='diagnosis',\n",
        "                 palette={0: 'skyblue', 1: 'salmon'}, corner=True)\n",
        "    # Adjust title positioning if using sns.pairplot's figure\n",
        "    plt.gcf().suptitle(f\"Pairplot of Top {len(top_features_for_pairplot)} Correlated Features (Training Data)\", y=1.03)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No features selected for pairplot based on correlation, skipping.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DOvDRIyXXQl"
      },
      "source": [
        "In the scatter matrix, look at the plots for pairs of features. You're hoping to see that malignant and benign points separate out.\n",
        "\n",
        "### **Box Plots by Diagnosis (Training Data)**\n",
        "\n",
        "Comparing feature distributions for benign vs. malignant cases using box plots on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWtr4-q7XYWF"
      },
      "outputs": [],
      "source": [
        "# Select a subset of features for clearer boxplots.\n",
        "if 'top_features_for_pairplot' in locals() and top_features_for_pairplot and len(top_features_for_pairplot) > 0 :\n",
        "    # Take distinct features, ensuring they exist in X_train_with_target\n",
        "    base_features = [f for f in top_features_for_pairplot if f in X_train_with_target.columns]\n",
        "    additional_features = [f for f in ['mean texture', 'mean area', 'mean smoothness'] if f in X_train_with_target.columns and f not in base_features]\n",
        "    features_for_boxplot_train = (base_features + additional_features)[:6] # Cap at 6 features for readability\n",
        "else:\n",
        "    # Fallback if top_features_for_pairplot wasn't properly defined or is empty\n",
        "    features_for_boxplot_train = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean concavity']\n",
        "    features_for_boxplot_train = [f for f in features_for_boxplot_train if f in X_train_with_target.columns][:6] # Ensure they exist and cap\n",
        "\n",
        "if not features_for_boxplot_train: # Ultimate fallback if list is still empty\n",
        "    print(\"Error: No valid features selected for boxplot. Using minimal default example.\")\n",
        "    features_for_boxplot_train = [col for col in ['mean radius', 'mean texture'] if col in X_train_with_target.columns]\n",
        "\n",
        "\n",
        "n_box_features = len(features_for_boxplot_train)\n",
        "if n_box_features > 0: # Proceed only if there are features to plot\n",
        "    n_box_cols = min(3, n_box_features) # Max 3 columns for boxplots\n",
        "    n_box_rows = (n_box_features + n_box_cols - 1) // n_box_cols # Calculate rows needed\n",
        "\n",
        "    plt.figure(figsize=(n_box_cols * 5, n_box_rows * 4)) # Setup figure size\n",
        "    for i, feature in enumerate(features_for_boxplot_train):\n",
        "        plt.subplot(n_box_rows, n_box_cols, i + 1) # Create subplot for each feature\n",
        "        # sns.boxplot shows distribution summaries.\n",
        "        # x='diagnosis' groups data by diagnosis.\n",
        "        # y=feature specifies the feature to plot.\n",
        "        # hue='diagnosis' and palette provide coloring. legend=False as x-axis is clear.\n",
        "        sns.boxplot(x='diagnosis', y=feature, data=X_train_with_target,\n",
        "                    hue='diagnosis', palette={0: 'skyblue', 1: 'salmon'}, legend=False)\n",
        "        plt.title(feature) # Title for each subplot\n",
        "\n",
        "    plt.suptitle(\"Boxplots of Selected Features by Diagnosis (Training Data: 0=Benign, 1=Malignant)\", fontsize=16, y=1.02) # Overall title\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust layout to prevent overlap\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No features available for boxplotting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkodIB-sXZNk"
      },
      "source": [
        "This plot should show some decent separation, with malignant tumors generally having higher values.\n",
        "\n",
        "**Experiment with Attribute Combinations** (Feature Engineering)\n",
        "\n",
        "Sometimes, creating new features from existing ones can help. For this dataset, the features are already quite meaningful. But for illustration, if we had `total_area` and `num_cells`, a `area_per_cell` might be useful.\n",
        "For now, we'll stick with the original features as they are well-established for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4KGvclXafU"
      },
      "source": [
        "---\n",
        "# 5. Prepare the Data for Machine Learning Algorithms\n",
        "\n",
        "Time to get the data ready! We need to handle:\n",
        "*   Missing values (our dataset is clean, but we'll show how).\n",
        "*   Categorical features (we don't have any, but we'll show how).\n",
        "*   Feature Scaling (very important!).\n",
        "\n",
        "We'll use Scikit-Learn's `Pipeline` to make this process neat and reproducible.\n",
        "\n",
        "### **Handling Missing Values (Illustrative)**\n",
        "Our dataset doesn't have missing values. But if it did, `SimpleImputer` is a good tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av_rsYhvXcAY"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer # For handling missing values\n",
        "\n",
        "# Create a temporary copy of X_train and introduce some NaNs for demonstration\n",
        "X_train_temp_for_imputation = X_train.copy()\n",
        "# Introduce some NaNs into the 'mean texture' column for the first 5 samples\n",
        "for i in range(min(5, len(X_train_temp_for_imputation))): # Ensure we don't go out of bounds\n",
        "    X_train_temp_for_imputation.iloc[i, X_train_temp_for_imputation.columns.get_loc('mean texture')] = np.nan\n",
        "\n",
        "print(\"NaNs in 'mean texture' (temp df) before imputation:\", X_train_temp_for_imputation['mean texture'].isnull().sum())\n",
        "\n",
        "# Initialize SimpleImputer with median strategy\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# Fit the imputer ONLY on the column(s) with NaNs from the training data copy\n",
        "imputer.fit(X_train_temp_for_imputation[['mean texture']])\n",
        "# Transform the column(s), replacing NaNs with the learned median\n",
        "X_train_temp_for_imputation['mean texture'] = imputer.transform(X_train_temp_for_imputation[['mean texture']])\n",
        "\n",
        "print(\"NaNs in 'mean texture' (temp df) AFTER imputation:\", X_train_temp_for_imputation['mean texture'].isnull().sum())\n",
        "print(\"First 5 'mean texture' values (temp df) after imputation with median:\")\n",
        "print(X_train_temp_for_imputation['mean texture'].head())\n",
        "print(\"\\n(End of illustrative imputation. We'll use the original clean X_train going forward.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enx0odlTXc5E"
      },
      "source": [
        "### **Handling Categorical Attributes (Illustrative)**\n",
        "\n",
        "We don't have categorical features here. But if we did, like `tumor_grade` ('G1', 'G2', 'G3'), we'd use `OneHotEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihl4_UYUXd1g"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder # For converting categorical features to numerical\n",
        "\n",
        "# Create dummy categorical data for illustration\n",
        "# Make sure the index aligns if you were to merge this back, using X_train.index\n",
        "if len(X_train) >= 5:\n",
        "    X_train_cat_example = pd.DataFrame({'tumor_grade': ['G1', 'G2', 'G3', 'G2', 'G1']},\n",
        "                                     index=X_train.index[:5])\n",
        "\n",
        "    # Initialize OneHotEncoder\n",
        "    # sparse_output=False returns a dense NumPy array.\n",
        "    # handle_unknown='ignore' makes the encoder robust to new categories in test data.\n",
        "    cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    # Fit and transform the dummy categorical data\n",
        "    tumor_grade_1hot = cat_encoder.fit_transform(X_train_cat_example[['tumor_grade']])\n",
        "\n",
        "    print(\"\\nOne-hot encoding example for 'tumor_grade':\")\n",
        "    print(tumor_grade_1hot)\n",
        "    print(f\"Categories found by OneHotEncoder: {cat_encoder.categories_}\")\n",
        "    # get_feature_names_out is the modern way to get output feature names\n",
        "    print(f\"Feature names out from OneHotEncoder: {cat_encoder.get_feature_names_out(['tumor_grade'])}\")\n",
        "else:\n",
        "    print(\"\\nSkipping illustrative OneHotEncoding as X_train has less than 5 samples.\")\n",
        "print(\"\\n(End of illustrative one-hot encoding.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvcE17FHXert"
      },
      "source": [
        "### **Feature Scaling**\n",
        "\n",
        "*Most* ML algorithms work better when numerical features are on a similar scale. We'll use `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWftr6y_XgK9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler # For standardizing features (mean=0, std=1)\n",
        "from sklearn.pipeline import Pipeline # For chaining preprocessing steps and models\n",
        "\n",
        "# We will define the scaler as the first step within each model's pipeline later.\n",
        "# This ensures that scaling is learned only from training data within cross-validation folds.\n",
        "# For demonstration, let's apply it to X_train to see its effect.\n",
        "num_pipeline_example = Pipeline([\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "X_train_prepared_np_example = num_pipeline_example.fit_transform(X_train)\n",
        "\n",
        "print(f\"\\nShape of X_train after example scaling: {X_train_prepared_np_example.shape}\")\n",
        "print(\"A few values from X_train after example scaling (first 2 rows, first 5 cols):\")\n",
        "print(X_train_prepared_np_example[:2, :5])\n",
        "\n",
        "# Convert back to DataFrame to check mean and std\n",
        "X_train_prepared_df_example = pd.DataFrame(X_train_prepared_np_example, columns=X_train.columns, index=X_train.index)\n",
        "print(\"\\nMean of 'mean radius' after example scaling:\", X_train_prepared_df_example['mean radius'].mean())\n",
        "print(\"Std dev of 'mean radius' after example scaling:\", X_train_prepared_df_example['mean radius'].std())\n",
        "print(\"\\n(Actual scaling will happen inside model pipelines on the fly.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvKCpADXhtl"
      },
      "source": [
        "---\n",
        "# 6. Select and Train a Model\n",
        "\n",
        "Now we select and train a few different classification models.\n",
        "\n",
        "Each model will be part of a pipeline that includes scaling as the first step.\n",
        "\n",
        "### **Baseline Model: Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU_fJXlgXjS2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # Logistic Regression classifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score # Evaluation metrics\n",
        "\n",
        "# Define the pipeline: 1. Scale data, 2. Apply Logistic Regression\n",
        "log_reg_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()), # First step: standard scaler\n",
        "    (\"log_reg\", LogisticRegression(solver=\"liblinear\", random_state=42)) # Second step: logistic regression model\n",
        "])\n",
        "# Fit the pipeline on the training data\n",
        "log_reg_pipeline.fit(X_train, y_train) # Scaler is fit_transformed on X_train, then model is fit\n",
        "\n",
        "# Evaluate on the TRAINING set (for an initial feel - cross-validation is more robust)\n",
        "y_train_pred_log_reg = log_reg_pipeline.predict(X_train) # Predict class labels\n",
        "train_accuracy_log_reg = accuracy_score(y_train, y_train_pred_log_reg) # Calculate accuracy\n",
        "train_probas_log_reg = log_reg_pipeline.predict_proba(X_train)[:, 1] # Probabilities for the positive class (Malignant=1)\n",
        "train_auc_log_reg = roc_auc_score(y_train, train_probas_log_reg) # Calculate ROC AUC score\n",
        "print(f\"\\nLogistic Regression TRAINING accuracy: {train_accuracy_log_reg:.4f}\")\n",
        "print(f\"Logistic Regression TRAINING AUC: {train_auc_log_reg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTHjyGpXkda"
      },
      "source": [
        "High 90s for accuracy and AUC on training data is pretty good for a start!\n",
        "\n",
        "### **Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKSNIhUEXlJG"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier\n",
        "tree_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"tree_clf\", DecisionTreeClassifier(random_state=42)) # random_state for reproducibility\n",
        "])\n",
        "tree_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the TRAINING set\n",
        "y_train_pred_tree = tree_pipeline.predict(X_train)\n",
        "train_accuracy_tree = accuracy_score(y_train, y_train_pred_tree)\n",
        "train_probas_tree = tree_pipeline.predict_proba(X_train)[:, 1]\n",
        "train_auc_tree = roc_auc_score(y_train, train_probas_tree)\n",
        "print(f\"\\nDecision Tree TRAINING accuracy: {train_accuracy_tree:.4f}\") # Often 1.0 if unconstrained (overfitting)\n",
        "print(f\"Decision Tree TRAINING AUC: {train_auc_tree:.4f}\") # Often 1.0 if unconstrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA0H6k8XmVy"
      },
      "source": [
        "A perfect score on training data for the Decision Tree is a big sign of overfitting!\n",
        "\n",
        "### **Better Evaluation Using Cross-Validation**\n",
        "Cross-validation gives a more robust estimate of model performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAm1ESDqXoRN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score # Function for cross-validation\n",
        "\n",
        "# Cross-validation for Logistic Regression pipeline\n",
        "# cv=10 means 10-fold cross-validation. scoring=\"roc_auc\" specifies the metric.\n",
        "log_reg_auc_scores_cv = cross_val_score(log_reg_pipeline, X_train, y_train, cv=10, scoring=\"roc_auc\")\n",
        "print(f\"\\nLogistic Regression 10-fold CV AUC scores: {log_reg_auc_scores_cv.round(4)}\")\n",
        "print(f\"Mean CV AUC: {log_reg_auc_scores_cv.mean():.4f}, Std CV AUC: {log_reg_auc_scores_cv.std():.4f}\")\n",
        "\n",
        "# Cross-validation for Decision Tree pipeline\n",
        "tree_auc_scores_cv = cross_val_score(tree_pipeline, X_train, y_train, cv=10, scoring=\"roc_auc\")\n",
        "print(f\"\\nDecision Tree 10-fold CV AUC scores: {tree_auc_scores_cv.round(4)}\")\n",
        "print(f\"Mean CV AUC: {tree_auc_scores_cv.mean():.4f}, Std CV AUC: {tree_auc_scores_cv.std():.4f}\")\n",
        "# Decision Tree CV score is usually much lower than its training score, indicating overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_hwXmX1XpDb"
      },
      "source": [
        "The Decision Tree's cross-validation AUC will be much lower than its training AUC, confirming overfitting. Logistic Regression's CV score is likely more stable and realistic.\n",
        "\n",
        "### **Random Forest Classifier**\n",
        "Random Forest is an ensemble model (multiple decision trees) that often performs well and is more robust to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFNOCjZJXqTr"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # Random Forest classifier\n",
        "forest_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"forest_clf\", RandomForestClassifier(n_estimators=100, random_state=42)) # n_estimators = number of trees\n",
        "])\n",
        "forest_auc_scores_cv = cross_val_score(forest_pipeline, X_train, y_train, cv=10, scoring=\"roc_auc\")\n",
        "print(f\"\\nRandom Forest 10-fold CV AUC scores: {forest_auc_scores_cv.round(4)}\")\n",
        "print(f\"Mean CV AUC: {forest_auc_scores_cv.mean():.4f}, Std CV AUC: {forest_auc_scores_cv.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_AUpKAXr4r"
      },
      "source": [
        "---\n",
        "# 7. Fine-Tune Your Model\n",
        "\n",
        "Let's assume Random Forest is our most promising model. Now we fine-tune its hyperparameters.\n",
        "\n",
        "**Grid Search (`GridSearchCV`)**\n",
        "Tries all combinations of hyperparameters you specify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TOH8LK6Xs0i"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV # For hyperparameter tuning\n",
        "\n",
        "# Define the parameter grid for RandomForestClassifier within our pipeline\n",
        "# Parameters are referred to by 'stepname__parametername'\n",
        "param_grid_rf = [\n",
        "    {'forest_clf__n_estimators': [50, 100, 150],        # Number of trees\n",
        "     'forest_clf__max_features': ['sqrt', 'log2', None], # Max features to consider for a split ('auto' is teofn sqrt)\n",
        "     'forest_clf__max_depth': [None, 10, 20],           # Max depth of each tree (None=unlimited)\n",
        "     'forest_clf__min_samples_split': [2, 5, 10]}         # Min samples to split an internal node\n",
        "]\n",
        "\n",
        "# Use the same pipeline structure; GridSearchCV will set the parameters\n",
        "# It's good practice to define it clearly here for the search.\n",
        "forest_pipeline_for_grid = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"forest_clf\", RandomForestClassifier(random_state=42)) # Base model with random_state for forest\n",
        "])\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5: 5-fold cross-validation for each parameter combination\n",
        "# scoring='roc_auc': Evaluate using ROC AUC\n",
        "# return_train_score=True: Useful for diagnosing overfitting of specific param sets\n",
        "# verbose=1: Shows progress messages\n",
        "# n_jobs=-1: Use all available CPU cores to speed up the search\n",
        "grid_search_rf = GridSearchCV(forest_pipeline_for_grid, param_grid_rf,\n",
        "                              cv=5, scoring='roc_auc',\n",
        "                              return_train_score=True, verbose=1, n_jobs=-1)\n",
        "# Fit GridSearchCV on the training data (this can take some time)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGridSearchCV Best Parameters (Random Forest):\")\n",
        "print(grid_search_rf.best_params_) # The combination of parameters that gave the best CV score\n",
        "print(f\"Best CV AUC score from GridSearchCV: {grid_search_rf.best_score_:.4f}\") # The best CV score\n",
        "\n",
        "# The best_estimator_ attribute holds the pipeline fitted with the best parameters on the whole X_train\n",
        "best_rf_model_gs = grid_search_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSL1FnyXtq0"
      },
      "source": [
        "GridSearchCV can take time. If the hyperparameter space is very large, `RandomizedSearchCV` is often a better bet. It samples a fixed number of hyperparameter combinations from specified distributions.\n",
        "\n",
        "**(Optional) Randomized Search (`RandomizedSearchCV`)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqgz2NI0Xu82"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs_rf = {\n",
        "    'forest_clf__n_estimators': randint(low=50, high=250),\n",
        "    'forest_clf__max_features': ['sqrt', 'log2', None],\n",
        "    'forest_clf__max_depth': randint(low=5, high=35),\n",
        "    'forest_clf__min_samples_split': randint(low=2, high=11),\n",
        "    'forest_clf__min_samples_leaf': randint(low=1, high=11)\n",
        "}\n",
        "\n",
        "# Using the same forest_pipeline structure\n",
        "# Set n_jobs=-1 to use all available CPU cores for parallel processing, which can speed things up.\n",
        "# However, for full reproducibility across different machines/setups, n_jobs=1 might be preferred\n",
        "# if there are subtle issues with how random states are handled in parallel by underlying libraries.\n",
        "rnd_search_rf = RandomizedSearchCV(forest_pipeline,\n",
        "                                   param_distributions=param_distribs_rf,\n",
        "                                   n_iter=30, # Number of parameter settings that are sampled\n",
        "                                   cv=5, scoring='roc_auc',\n",
        "                                   random_state=42, verbose=1, return_train_score=True, n_jobs=-1)\n",
        "# To run RandomizedSearchCV:\n",
        "# rnd_search_rf.fit(X_train, y_train)\n",
        "# print(\"\\nRandomizedSearchCV Best Parameters (Random Forest):\")\n",
        "# print(rnd_search_rf.best_params_)\n",
        "# print(f\"Best CV AUC score from RandomizedSearchCV: {rnd_search_rf.best_score_:.4f}\")\n",
        "# best_rf_model_rs = rnd_search_rf.best_estimator_ # Update best model if you run this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPj1BUDdXvmx"
      },
      "source": [
        "For this demo, we'll proceed with the model from `GridSearchCV` (`best_rf_model_gs`).\n",
        "\n",
        "### **Analyzing the Best Models and Their Errors**\n",
        "Once you have your best model, you can inspect it. For Random Forests, feature importances tell you which features were most influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCuNCh3wXwwK"
      },
      "outputs": [],
      "source": [
        "# Let's use the best model from GridSearchCV\n",
        "final_model_pipeline = best_rf_model_gs # or best_rf_model_rs if you ran RandomizedSearch and it was better\n",
        "\n",
        "# The RandomForestClassifier is the step named \"forest_clf\" in our pipeline\n",
        "final_model_classifier_step = final_model_pipeline.named_steps[\"forest_clf\"]\n",
        "feature_importances = final_model_classifier_step.feature_importances_\n",
        "\n",
        "# Get feature names (they are the same as X_train.columns because scaler doesn't change them)\n",
        "feature_names = X_train.columns\n",
        "\n",
        "sorted_importances_with_names = sorted(zip(feature_importances, feature_names), reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 Feature Importances (from best Random Forest):\")\n",
        "for importance, name in sorted_importances_with_names[:10]:\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_n = 15\n",
        "# Plot horizontal bar chart (easier to read with many features)\n",
        "# We take top_n and then reverse the list for plotting so the most important is at the top\n",
        "names_for_plot = [name for imp, name in sorted_importances_with_names[:top_n]][::-1]\n",
        "importances_for_plot = [imp for imp, name in sorted_importances_with_names[:top_n]][::-1]\n",
        "\n",
        "plt.barh(names_for_plot, importances_for_plot)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(f\"Top {top_n} Feature Importances\")\n",
        "plt.tight_layout() # Adjust layout to make sure everything fits\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wcOt0zlXxev"
      },
      "source": [
        "This can give biological insights! For example, 'worst concave points' or 'worst radius' might show up as highly important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIwOEl96XyrA"
      },
      "source": [
        "---\n",
        "# 8. Evaluate Your System on the Test Set\n",
        "\n",
        "The moment of truth! We evaluate our final, tuned model (`final_model_pipeline`) on the `X_test` and `y_test` data that we set aside at the very beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXJhSHn8XzyG"
      },
      "outputs": [],
      "source": [
        "y_test_pred = final_model_pipeline.predict(X_test)\n",
        "y_test_proba = final_model_pipeline.predict_proba(X_test)[:, 1] # Probabilities for the positive class (Malignant=1)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "print(f\"\\nFinal Model Performance on Test Set:\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# You can also look at a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\nConfusion Matrix on Test Set (TN, FP / FN, TP):\")\n",
        "print(conf_matrix)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Benign (0)', 'Malignant (1)'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# And plot the ROC curve for the test set\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {test_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) - Test Set')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3mnV-KxX0gQ"
      },
      "source": [
        "The test set performance is your best estimate of how your model will perform on new, unseen data. Resist the temptation to tweak your model further based on these test set results!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNSavIZ8X1R6"
      },
      "source": [
        "---\n",
        "# 9. Launch, Monitor, and Maintain Your System (Briefly)\n",
        "\n",
        "If this were a real-world application:\n",
        "\n",
        "**Save Your Model**: Use `joblib` to save your trained pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFFgQg71X2Ld"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "model_filename = \"breast_cancer_classification_model.pkl\"\n",
        "joblib.dump(final_model_pipeline, model_filename)\n",
        "print(f\"\\nModel saved as {model_filename}\")\n",
        "\n",
        "# To load it later:\n",
        "# loaded_model = joblib.load(model_filename)\n",
        "# predictions = loaded_model.predict(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xaXqBwqX2wG"
      },
      "source": [
        "*   **Deploy**: This could mean wrapping it in a web service, integrating it into lab software, etc.\n",
        "*   **Monitor**: Continuously check its performance on new data. Monitor for data drift or concept drift. For medical uses, alerts for changes in error rates (especially false negatives) are critical.\n",
        "*   **Maintain**: Regularly retrain your model on fresh data. Keep backups of datasets and model versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HNxIedlX30a"
      },
      "source": [
        "---\n",
        "And that's a walkthrough of an end-to-end machine learning project. The key is a systematic approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwWz5GZXnZz2rLQxedxawR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}